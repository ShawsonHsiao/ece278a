{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b67dce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Active and Volumetric Stereo</center>\n",
    "#### <center>Presented by: Maxwell Jung</center>\n",
    "\n",
    "Captured Image                                      | 3D Reconstruction\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/hand_fist_pattern.jpg\" alt=\"hand_fist_pattern\" width=\"500\"/> | <img src=\"img/hand_fist_polygon.gif\" alt=\"hand_fist_polygon\" width=\"500\"/>\n",
    "\n",
    "Source: [Rapid Shape Acquisition Using Color Structured Light and Multi-pass Dynamic Programming](https://grail.cs.washington.edu/projects/moscan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0b2b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. Introduce Active Stereo\n",
    "    1. Point method\n",
    "    2. Line method\n",
    "    3. Pattern method\n",
    "2. Demo Active Stereo\n",
    "3. Introduce Volumetric Stereo\n",
    "    1. Space Carving\n",
    "    2. Shadow Carving\n",
    "    3. Voxel Coloring\n",
    "4. Demo Volumetric Stereo\n",
    "5. Recent papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c97ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples\n",
    "\n",
    "Captured Image (line method)                        | 3D Reconstruction\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/digitalmichelangelo.jpg\" alt=\"digitalmichelangelo\" width=\"500\"/> | <img src=\"img/david3d.jpg\" alt=\"david3d\" width=\"500\"/>\n",
    "\n",
    "Source: [The Digital Michaelangelo project](https://accademia.stanford.edu/mich/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af1f04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples - cont.\n",
    "\n",
    "Captured Image (pattern method)                       | 3D Reconstruction\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/speckle-a.png\" alt=\"speckle-a\" width=\"500\"/> | <img src=\"img/speckle-b.png\" alt=\"speckle-b\" width=\"500\"/>\n",
    "\n",
    "Source: [Rapid Shape Acquisition Using Color Structured Light and Multi-pass Dynamic Programming](https://grail.cs.washington.edu/projects/moscan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6e9c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples - cont.\n",
    "\n",
    "Captured Images (Space Carving)                       | 3D Reconstruction\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/space-carving-hands.jpeg\" alt=\"space-carving-hands\" width=\"500\"/> | <img src=\"img/space-carving-hands-result.png\" alt=\"space-carving-hands-result\" width=\"500\"/>\n",
    "\n",
    "Source: [A Theory of Shape by Space Carving](https://www.cs.toronto.edu/~kyros/pubs/00.ijcv.carve.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c48d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Traditional Stereo - Triangulation\n",
    "\n",
    "Given\n",
    "  - Pixels $p$ and $p'$ corresponding to the same 3D point $P$\n",
    "  - Calibrated Cameras (intrisic and extrinsic properties are known)\n",
    "\n",
    "Outcome\n",
    "  - Location of point $P$\n",
    "\n",
    "<img src=\"img/traditional-stereo.png\" alt=\"traditional-stereo\" width=\"600\"/>\n",
    "\n",
    "<center>Source: <a href=\"https://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture8_volumetric_stereo.pdf\">Stanford CS231A lecture 8</a></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ccc6d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Traditional Stereo - Correspondence Problem\n",
    "\n",
    "How to ensure pixels $p$ and $p'$ **correspond** to the same 3D point $P$?\n",
    "\n",
    "Easy for humans, very difficult for computers - Sparse vs Dense correspondence\n",
    "\n",
    "<img src=\"img/correspondence.png\" alt=\"correspondence\" width=\"1000\"/>\n",
    "\n",
    "<center>Source: <a href=\"https://srs.amsi.org.au/student-blog/what-is-dense-correspondence/\">SRS - What is Dense Correspondence?</a></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef60ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution: Active Stereo (Point)\n",
    "\n",
    "- Physically mark the object with an identifiable dot\n",
    "    - Replace camera and image plane with projector and projector virtual plane\n",
    "- Reconstruction\n",
    "    1. Scan the dot across the surface of the object (ideally one pixel per dot)\n",
    "        - 307200 unique images required to reconstruct 640 $\\times$ 480 resolution image\n",
    "    2. Calculate coordinate of each dot using triangulation\n",
    "\n",
    "<img src=\"img/active-point.png\" alt=\"active-point\" width=\"600\"/>\n",
    "\n",
    "<center>Source: <a href=\"https://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture8_volumetric_stereo.pdf\">Stanford CS231A lecture 8</a></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609509c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo (Line)\n",
    "\n",
    "- Speeds up Point method by projecting a line instead of a single dot\n",
    "- Reconstruction\n",
    "    1. Scan line across the surface of the object (ideally one line per row/column)\n",
    "        - 480 images required to reconstruct 640 $\\times$ 480 resolution image\n",
    "    2. Calculate intersection of line and plane to reconstruct depth of each pixel on the line\n",
    "\n",
    "<img src=\"img/active-line.png\" alt=\"active-line\" width=\"600\"/>\n",
    "\n",
    "<center>Source: <a href=\"https://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture8_volumetric_stereo.pdf\">Stanford CS231A lecture 8</a></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25e508",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo (Line) - Math\n",
    "\n",
    "<img src=\"img/active-line-math.png\" alt=\"active-line-math\" width=\"500\"/>\n",
    "\n",
    "Source: [First Principles of Computer Vision](https://youtu.be/3S3xLUXAgHw?si=-YDtpRHqVZD3M_Bx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c47e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo (Line) - Math\n",
    "\n",
    "Equation of line - start from $l_o$ and move in the direction of $l$\n",
    "\n",
    "$p = l_o + ld$\n",
    "\n",
    "Equation of plane - all points perpendicular to some normal vector $n$\n",
    "\n",
    "$(p - p_o) \\cdot n = 0$\n",
    "\n",
    "Substitue $p$ from the line to the plane\n",
    "\n",
    "$(l_o + ld - p_o) \\cdot n = 0$\n",
    "\n",
    "$d = \\frac{(p_o - l_o) \\cdot n}{l \\cdot n}$\n",
    "\n",
    "note that $l_o$ is the camera origin $(0, 0, 0)$, so\n",
    "\n",
    "$\\frac{(p_o - l_o) \\cdot n}{l \\cdot n} = \\frac{p_o \\cdot n}{l \\cdot n}$\n",
    "\n",
    "turns out, for a plane with equation, $Ax + Bx + Cx + D = 0$\n",
    "\n",
    "$\\frac{p_o \\cdot n}{l \\cdot n} = \\frac{D}{Ax + By + Cf}$\n",
    "\n",
    "\n",
    "Source: [Wikipedia: Line-plane intersection](https://en.wikipedia.org/wiki/Line%E2%80%93plane_intersection#:~:text=In%20analytic%20geometry%2C%20the%20intersection,the%20plane%20but%20outside%20it.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95946639",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo (Line) - Example\n",
    "\n",
    "Laser Scanning                        | 3D Reconstruction\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/digitalmichelangelo.jpg\" alt=\"digitalmichelangelo\" width=\"500\"/> | <img src=\"img/david3d.jpg\" alt=\"david3d\" width=\"500\"/>\n",
    "\n",
    "Source: [The Digital Michaelangelo project](https://accademia.stanford.edu/mich/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104b0e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo (Pattern)\n",
    "\n",
    "- Extension of the Active Stereo Point method\n",
    "    - Project multiple dots/lines covering a wide area\n",
    "        - Correspondence issue\n",
    "            - solution: Binary Coded Structured Light or unique colors\n",
    "- Reconstruction\n",
    "    1. Calculate coordinates of each dot/line using triangulation\n",
    "    2. Generate mesh from point cloud\n",
    "\n",
    "<img src=\"img/active-pattern.png\" alt=\"active-pattern\" width=\"600\"/>\n",
    "\n",
    "<center>Source: <a href=\"https://cvgl.stanford.edu/teaching/cs231a_winter1415/lecture/lecture8_volumetric_stereo.pdf\">Stanford CS231A lecture 8</a></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c060940",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo (Pattern) - Example\n",
    "\n",
    "- Microsoft Kinect for Xbox\n",
    "- Project pseudo-random pattern of infrared dots\n",
    "    - psueo-random pattern guarantees unique correspondence by comparing neighboring patterns\n",
    "    - works well under any ambient light conditions\n",
    "\n",
    "<img src=\"img/kinect.png\" alt=\"kinect\" width=\"600\"/>\n",
    "\n",
    "<center>Source: <a href=\"https://www.mdpi.com/2220-9964/6/11/349\">A Post-Rectification Approach of Depth Images of Kinect v2 for 3D Reconstruction of Indoor Scenes</a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529dd1fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active Stereo Demo\n",
    "\n",
    "1. Capture images\n",
    "2. Apply algorithm on captured images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c56f03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate Active Stereo Line method using open3d\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Import Stanford Bunny 3D mesh\n",
    "bunny_mesh = o3d.data.BunnyMesh()\n",
    "mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)\n",
    "\n",
    "# Center mesh to origin\n",
    "mesh = mesh.translate(-mesh.get_center())\n",
    "# Normalize mesh to radius 1 bounding box\n",
    "mesh = mesh.scale(scale=2/(max(mesh.get_minimal_oriented_bounding_box().get_max_bound())\n",
    "                           - min(mesh.get_minimal_oriented_bounding_box().get_min_bound())),\n",
    "                  center=[0, 0, 0])\n",
    "\n",
    "# Visualize mesh\n",
    "mesh.compute_vertex_normals()\n",
    "o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "# Create a scene and add the bunny mesh\n",
    "scene = o3d.t.geometry.RaycastingScene()\n",
    "bunny = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n",
    "bunny_id = scene.add_triangles(bunny)\n",
    "\n",
    "# Create laser scanner by horizontally stacking 100,000 raycasts\n",
    "rays = o3d.t.geometry.RaycastingScene.create_rays_pinhole(\n",
    "    fov_deg=90,\n",
    "    center=[0, -0.2, 0], # Point scanner at (0, -0.2, 0)\n",
    "    eye=[1, 1, 1], # Place laser scanner on (1, 1, 1)\n",
    "    up=[0, -1, 0],\n",
    "    width_px=100_000,\n",
    "    height_px=1,\n",
    ")\n",
    "rays = rays.reshape((rays.shape[0]*rays.shape[1], 6))\n",
    "\n",
    "# Compute the ray intersections.\n",
    "lx = scene.list_intersections(rays)\n",
    "lx = {k:v.numpy() for k,v in lx.items()}\n",
    "\n",
    "# Calculate intersection coordinates using ray_ids\n",
    "c = rays[lx['ray_ids']][:,:3] + rays[lx['ray_ids']][:,3:]*lx['t_hit'][...,None]\n",
    "intersections_pc = o3d.t.geometry.PointCloud(positions=c)\n",
    "\n",
    "# Visualize the rays and intersections\n",
    "lasers = o3d.t.geometry.LineSet()\n",
    "ray_orig = rays[:, :3].numpy()\n",
    "ray_dest = ray_orig + rays[:, 3:].numpy()\n",
    "lasers.point.positions = np.hstack([ray_orig,ray_dest]).reshape(-1,3)\n",
    "lasers.line.indices = np.arange(lasers.point.positions.shape[0]).reshape(-1,2)\n",
    "lasers.line.colors = np.full((lasers.line.indices.shape[0],3), (1,0,0)) # red color\n",
    "\n",
    "# Visualize with lasers\n",
    "geometry_list = [\n",
    "    o3d.t.geometry.TriangleMesh.to_legacy(bunny), \n",
    "    o3d.t.geometry.LineSet.to_legacy(lasers), \n",
    "    o3d.t.geometry.PointCloud.to_legacy(intersections_pc).paint_uniform_color([1, 0, 0]) # red color\n",
    "]\n",
    "o3d.visualization.draw_geometries(geometry_list)\n",
    "\n",
    "# Visualize without lasers\n",
    "geometry_list = [\n",
    "    o3d.t.geometry.TriangleMesh.to_legacy(bunny), \n",
    "    o3d.t.geometry.PointCloud.to_legacy(intersections_pc).paint_uniform_color([1, 0, 0]) # red color\n",
    "]\n",
    "\n",
    "\n",
    "o3d.visualization.draw_geometries(geometry_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d14c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Import Stanford Bunny 3D mesh\n",
    "bunny_mesh = o3d.data.BunnyMesh()\n",
    "mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)\n",
    "\n",
    "# Center mesh to origin\n",
    "mesh = mesh.translate(-mesh.get_center())\n",
    "# Normalize mesh to radius 1 bounding box\n",
    "mesh = mesh.scale(scale=2/(max(mesh.get_minimal_oriented_bounding_box().get_max_bound())\n",
    "                           - min(mesh.get_minimal_oriented_bounding_box().get_min_bound())),\n",
    "                  center=[0, 0, 0])\n",
    "mesh.compute_vertex_normals()\n",
    "\n",
    "# Create a scene and add the bunny mesh\n",
    "scene = o3d.t.geometry.RaycastingScene()\n",
    "bunny = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n",
    "bunny_id = scene.add_triangles(bunny)\n",
    "\n",
    "def scan_once(vis, y, foldername='dataset/bunny-scans', filename=f'scan'):\n",
    "    rays = o3d.t.geometry.RaycastingScene.create_rays_pinhole(\n",
    "        fov_deg=90,\n",
    "        center=[0, y, 0], # Point scanner towards (0, 0, 0)\n",
    "        eye=[1, 1, 1], # Place laser scanner at (1, 1, 1)\n",
    "        up=[0, -1, 0],\n",
    "        width_px=100_000, # Stack 100,000 raycasts horizontally to form a plane\n",
    "        height_px=1,\n",
    "    )\n",
    "    rays = rays.reshape((rays.shape[0]*rays.shape[1], 6))\n",
    "    \n",
    "    # Compute the ray intersections.\n",
    "    lx = scene.list_intersections(rays)\n",
    "    lx = {k:v.numpy() for k,v in lx.items()}\n",
    "\n",
    "    # Calculate intersection coordinates using ray_ids\n",
    "    c = rays[lx['ray_ids']][:,:3] + rays[lx['ray_ids']][:,3:]*lx['t_hit'][...,None]\n",
    "    intersections_pc = o3d.t.geometry.PointCloud(positions=c)\n",
    "    \n",
    "    # Add bunny and intersections to visualizer\n",
    "    vis.clear_geometries()\n",
    "    vis.add_geometry(o3d.t.geometry.TriangleMesh.to_legacy(bunny))\n",
    "    if intersections_pc.point.positions.shape[0] > 0:\n",
    "        vis.add_geometry(o3d.t.geometry.PointCloud.to_legacy(intersections_pc).paint_uniform_color([1, 0, 0]))\n",
    "\n",
    "    # Write image to disk\n",
    "    if not os.path.exists(foldername):\n",
    "        os.makedirs(foldername)\n",
    "    vis.capture_screen_image(f\"{foldername}/{filename}.png\", do_render=True)\n",
    "    print(f'Captured {foldername}/{filename}.png')\n",
    "\n",
    "vertical_resolution = 480\n",
    "bunny_scans = []\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(width=680, height=vertical_resolution, left=0, top=0, visible=False)\n",
    "vis.get_render_option().point_size = 2\n",
    "for i in range(vertical_resolution):\n",
    "    # evenly scan 3D mesh from y = -1 to 1\n",
    "    scan_once(vis, y=-1 + 2 * (i/vertical_resolution), filename=i)\n",
    "\n",
    "camera_params = vis.get_view_control().convert_to_pinhole_camera_parameters()\n",
    "extrinsic = camera_params.extrinsic\n",
    "intrinsic = camera_params.intrinsic.intrinsic_matrix\n",
    "focal = camera_params.intrinsic.get_focal_length()\n",
    "\n",
    "print(f'{extrinsic=}')\n",
    "print(f'{intrinsic=}')\n",
    "print(f'{focal=}')\n",
    "\n",
    "np.save(\"dataset/bunny-scans/extrinsic\", extrinsic)\n",
    "np.save(\"dataset/bunny-scans/intrinsic\", intrinsic)\n",
    "\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360e7bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import skimage as ski\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "# Read all images in folder\n",
    "dataset_dir = 'dataset/bunny/*.png'\n",
    "dataset = ski.io.imread_collection(dataset_dir)\n",
    "vertical_resolution = len(dataset)\n",
    "\n",
    "# Display 5 random images\n",
    "for i in range(5):\n",
    "    img = random.choice(dataset)\n",
    "    ski.io.imshow(img)\n",
    "    ski.io.show()\n",
    "\n",
    "extrinsic = np.load(\"dataset/bunny/extrinsic.npy\")\n",
    "intrinsic = np.load(\"dataset/bunny/intrinsic.npy\")\n",
    "\n",
    "f_x = intrinsic[0,0]\n",
    "f_y = intrinsic[1,1]\n",
    "pixels_per_meter = 170 # this value needs verification\n",
    "f = f_x/pixels_per_meter\n",
    "o_x = intrinsic[0,2]\n",
    "o_y = intrinsic[1,2]\n",
    "\n",
    "def pixel_to_cam_x(u, z):\n",
    "    return (u - o_x) * z/f_x\n",
    "\n",
    "def pixel_to_cam_y(v, z):\n",
    "    return (v - o_y) * z/f_y\n",
    "\n",
    "def get_plane_eq(u, v):\n",
    "    n = np.cross(v, u)\n",
    "    n_hat = n / np.linalg.norm(n)\n",
    "    \n",
    "    # equation of plane\n",
    "    # n_x(x - o_x) + n_y(y - o_y) + n_z(z - o_z) = 0\n",
    "    # (o_x, o_y, o_z) = (1, 1, 1)\n",
    "    # n_x(x) - n_x(o_x) + n_y(y) - n_y(o_y) + n_z(z) - n_z(o_z) = 0\n",
    "    # n_x(x) + n_y(y) + n_z(z) - n_x(o_x) - n_y(o_y) - n_z(o_z) = 0\n",
    "    A = n_hat[0]\n",
    "    B = n_hat[1]\n",
    "    C = n_hat[2]\n",
    "    D = - (n_hat @ projector_origin)\n",
    "    \n",
    "    return A, B, C, D\n",
    "\n",
    "def get_red_pixel_coords(img):\n",
    "    result = img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower = np.array([0,50,0])\n",
    "    upper = np.array([179,255,255])\n",
    "    mask = cv2.inRange(img, lower, upper)\n",
    "    px, py = np.where(mask!=0)\n",
    "    return px, py\n",
    "\n",
    "projector_origin = extrinsic.dot(np.array([1, 1, 1, 1]))[:3]\n",
    "print(projector_origin)\n",
    "points = []\n",
    "for i in range(vertical_resolution):\n",
    "    projector_dest = extrinsic.dot(np.array([0, -1 + 2 * (i/vertical_resolution), 0 , 1]))[:3]\n",
    "    u = projector_dest - projector_origin\n",
    "    v = extrinsic.dot([1, 0, -1, 1])[:3]\n",
    "    \n",
    "    A, B, C, D = get_plane_eq(u, v)\n",
    "    \n",
    "    img = ski.util.img_as_ubyte(dataset[i])\n",
    "    py, px = get_red_pixel_coords(img)\n",
    "    \n",
    "    if len(px) == 0:\n",
    "        continue\n",
    "    \n",
    "    x = pixel_to_cam_x(px, 1)\n",
    "    y = pixel_to_cam_y(py, 1)\n",
    "    z = D/(A*x + B*y + C*f)\n",
    "    \n",
    "    xyz = np.stack((x, y, z)).T\n",
    "    points.append(xyz)\n",
    "\n",
    "points = np.vstack(points)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a99ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Volumetric Stereo\n",
    "\n",
    "- Work backwards from the 3D point to the individual projections\n",
    "    - Assume point $P$ is part of an enclosed object\n",
    "    - Calculate the expected projection of $P$ for each camera\n",
    "    - Check if the expected projection matches the actual projection (**consistency check**)\n",
    "    - Mark $P$ as part of the object\n",
    "\n",
    "<img src=\"img/vstereo.png\" alt=\"vstereo\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb45e46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Volumetric Stereo - Consistency Check\n",
    "\n",
    "- Space Carving\n",
    "    - Check if silhouettes match\n",
    "- Shadow Carving\n",
    "    - Check if shadows match\n",
    "- Voxel Coloring\n",
    "    - Check if colors match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ab192",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Space Carving - Intuition\n",
    "\n",
    "- Silhouettes provide useful 3D information\n",
    "\n",
    "Silhouette | Object\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/toy-dino-silhouette.png\" alt=\"toy-dino-silhouette\" width=\"500\"/> | <img src=\"img/toy-dino.png\" alt=\"toy-dino\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f68f90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Space Carving - Visual Cone\n",
    "\n",
    "- Create a cone in the shape of the silhouette starting from the camera origin and extending to infinity\n",
    "    - Volume covered by cone represents the set of points that can produce to the observed silhouette\n",
    "\n",
    "<img src=\"img/silhouette-cone.png\" alt=\"silhouette-cone\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f486d56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Space Carving - Visual Hull\n",
    "\n",
    "- Intersection of all Visual Cones\n",
    "- Represents the set of points that produces **all** observed silhouettes\n",
    "\n",
    "<img src=\"img/visual-hull.png\" alt=\"visual-hull\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349c941",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Space Carving - Voxels\n",
    "\n",
    "- 3D version of a pixel\n",
    "\n",
    "<img src=\"img/voxel.png\" alt=\"voxel\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ee46f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Space Carving - Voxel Implementation\n",
    "\n",
    "- Iterate over each voxel and check if each voxel is inside the Visual Hull\n",
    "    - $O(n^3)$ time complexity\n",
    "        - Use Octrees for complexity reduction\n",
    "\n",
    "<img src=\"img/voxel-carving.png\" alt=\"voxel-carving\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3e9e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Space Carving - Limitations\n",
    "\n",
    "- Need a lot of silhouettes at many different angles for high accuracy\n",
    "- Impossible to detect concavitiy\n",
    "\n",
    "<img src=\"img/concavity.png\" alt=\"concavity\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc025a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shadow Carving - Intuition\n",
    "\n",
    "- Concave surfaces often cast shadows onto itself\n",
    "    - Augment Space Carving with shadows to estimate concavity\n",
    "\n",
    "<img src=\"img/crater.png\" alt=\"crater\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd3621",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shadow Carving - Setup\n",
    "\n",
    "- Take multiple photos with different lighting conditions\n",
    "    - One light source per photo\n",
    "\n",
    "<img src=\"img/shadow-carving-setup.png\" alt=\"shadow-carving-setup\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb3286",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shadow Carving - Voxel Implementation\n",
    "\n",
    "1. Obtain upper bound estimate using Space Carving\n",
    "2. Project shadow onto upper bounded surface\n",
    "3. Project shadow from upper bounded surface to virtual light image\n",
    "4. Remove voxels that project to both image shadow and virtual image shadow\n",
    "- $O((k+1)n^3)$ time complexity for $k$ lights and $n \\times n \\times n$ voxel resolution\n",
    "\n",
    "<img src=\"img/shadow-carving.png\" alt=\"shadow-carving\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f245c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shadow Carving (vs. Space Carving)\n",
    "\n",
    "<img src=\"img/space-vs-shadow.png\" alt=\"space-vs-shadow\" width=\"600\"/>\n",
    "<img src=\"img/space-vs-shadow2.png\" alt=\"space-vs-shadow2\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b34de2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Voxel Coloring\n",
    "- Check if the color of voxels sufficiently match between projections\n",
    "- Advantage\n",
    "    - colored 3D reconstruction\n",
    "- Disadvantage\n",
    "    - cannot guarantee unique solution\n",
    "    - requires Lambertian surface (constant luminance across viewpoints)\n",
    "\n",
    "<img src=\"img/voxel-coloring.png\" alt=\"voxel-coloring\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db9d77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Voxel Coloring - Ambiguity\n",
    "\n",
    "- Different 3D reconstructions can produce the same 2D projections\n",
    "- solution: visibility constraint\n",
    "    - traverse voxels layer by layer starting from ones closest to camera\n",
    "    - ensure voxel is viewable by at least 2 cameras\n",
    "\n",
    "<img src=\"img/voxel-coloring-ambig.png\" alt=\"voxel-coloring-ambig\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80efa5e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Voxel Coloring - Results\n",
    "\n",
    "Input | Voxel Coloring Output\n",
    ":--------------------------------------------------:|:-------------------------:\n",
    "<img src=\"img/dino.gif\" alt=\"dino\" width=\"500\"/> | <img src=\"img/dino-3d.gif\" alt=\"dino-3d\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b38ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Volumetric Stereo - Demo\n",
    "\n",
    "1. Capture images\n",
    "2. Apply Space Carving Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Space Carving method using open3d\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Import Stanford Bunny 3D mesh\n",
    "bunny_mesh = o3d.data.BunnyMesh()\n",
    "mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)\n",
    "\n",
    "# Center mesh to origin\n",
    "mesh = mesh.translate(-mesh.get_center())\n",
    "# Normalize mesh to radius 1 bounding box\n",
    "mesh = mesh.scale(scale=2/(max(mesh.get_minimal_oriented_bounding_box().get_max_bound())\n",
    "                           - min(mesh.get_minimal_oriented_bounding_box().get_min_bound())),\n",
    "                  center=[0, 0, 0])\n",
    "\n",
    "# Visualize mesh\n",
    "mesh.compute_vertex_normals()\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture images and camera parameters\n",
    "images = []\n",
    "extrinsics = []\n",
    "intrinsics = []\n",
    "\n",
    "def custom_draw_geometry_with_key_callback(mesh):\n",
    "    def capture_image(vis):\n",
    "        image = vis.capture_screen_float_buffer()\n",
    "        camera_params = vis.get_view_control().convert_to_pinhole_camera_parameters()\n",
    "        extrinsic = camera_params.extrinsic\n",
    "        intrinsic = camera_params.intrinsic.intrinsic_matrix\n",
    "        \n",
    "        images.append(image)\n",
    "        extrinsics.append(extrinsic)\n",
    "        intrinsics.append(intrinsic)\n",
    "        \n",
    "        plt.imshow(np.asarray(image))\n",
    "        plt.show()\n",
    "        return False\n",
    "\n",
    "    key_to_callback = {}\n",
    "    key_to_callback[ord(\".\")] = capture_image\n",
    "    o3d.visualization.draw_geometries_with_key_callbacks([mesh], key_to_callback, width=640, height=480)\n",
    "    \n",
    "custom_draw_geometry_with_key_callback(mesh.paint_uniform_color([1, 0, 0]))\n",
    "\n",
    "images = np.array(images)\n",
    "extrinsics = np.array(extrinsics)\n",
    "intrinsics = np.array(intrinsics)\n",
    "\n",
    "print(images.shape)\n",
    "print(extrinsics.shape)\n",
    "print(intrinsics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a66a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import skimage as ski\n",
    "\n",
    "def get_silhouette(image):\n",
    "    imgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(imgray, 0.99, 1, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "silhouettes = []\n",
    "for i in range(len(images)):\n",
    "    image = ski.util.img_as_float(images[i])\n",
    "    silhouette = get_silhouette(image)\n",
    "    silhouette = ski.util.img_as_ubyte(silhouette)\n",
    "    \n",
    "    silhouettes.append(silhouette)\n",
    "    ski.io.imshow(silhouette)\n",
    "    ski.io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81036154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistency(coords, silhouette):\n",
    "    def is_consistent(x, y):\n",
    "        in_frame = (0 < x) & (x < silhouette_width) & (0 < y) & (y < silhouette_height)\n",
    "        if not in_frame: return False\n",
    "        return silhouette[y, x] > 0\n",
    "        \n",
    "    silhouette_height = silhouette.shape[0]\n",
    "    silhouette_width = silhouette.shape[1]\n",
    "    px = coords[:, 0]\n",
    "    py = coords[:, 1]\n",
    "    \n",
    "    return np.vectorize(is_consistent)(px, py)\n",
    "\n",
    "# Initialize Voxel\n",
    "voxel_count = 100j\n",
    "xyz = np.mgrid[-1:1:voxel_count, -1:1:voxel_count, -1:1:voxel_count].reshape(3,-1).T\n",
    "xyzw = np.hstack((xyz, np.ones([xyz.shape[0], 1], xyz.dtype)))\n",
    "\n",
    "visual_hull = np.ones(xyz.shape[0], bool)\n",
    "for i in range(len(images)):\n",
    "    silhouette = silhouettes[i]\n",
    "    \n",
    "    homog_coords = intrinsics[i].dot(extrinsics[i].dot(xyzw.T)[:3])\n",
    "    pixel_coords = (homog_coords[:2] / homog_coords[2]).T\n",
    "    pixel_coords = np.round(pixel_coords).astype(int)\n",
    "    \n",
    "    visual_cone = check_consistency(pixel_coords, silhouette)\n",
    "    visual_hull = visual_hull & visual_cone\n",
    "    print(f'Found {np.count_nonzero(visual_cone)} consistent voxels in {i}th silhouette')\n",
    "\n",
    "points = xyz[visual_hull]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
